% Encoding: UTF-8

@article{Xuan2022,
   abstract = {As robots rapidly enter society, how does human social cognition respond to their novel presence? Focusing on one foundational social-cognitive capacity—visual perspective taking—seven studies reveal that people spontaneously adopt a robot's unique perspective and do so with patterns of variation that mirror perspective taking toward humans. As they do with humans, people take a robot's visual perspective when it displays goal-directed actions. Moreover, perspective taking is absent when the agent lacks human appearance, increases when the agent looks highly humanlike, and persists even when the humanlike agent is perceived as eerie or as obviously lacking a mind. These results suggest that visual perspective taking toward robots is consistent with a “mere appearance hypothesis”—a form of stimulus generalization based on humanlike appearance—rather than following an “uncanny valley” pattern or arising from mind perception. Robots' superficial human resemblance may trigger and modulate social-cognitive responses in human observers originally developed for human interaction.},
   author = {Zhao Xuan and Malle Bertram F.},
   doi = {10.1016/j.cognition.2022.105076},
   issn = {18737838},
   journal = {Cognition},
   keywords = {Anthropomorphism,Artificial intelligence,Human-robot interaction,Perspective taking,Social cognition,Theory of Mind},
   month = {7},
   pmid = {35364401},
   publisher = {Elsevier B.V.},
   title = {Spontaneous perspective taking toward robots: The unique impact of humanlike appearance},
   volume = {224},
   year = {2022},
}
@article{Xiao2021,
   abstract = {Spatial communications are essential to the survival and social interaction of human beings. In science fiction and the near future, robots are supposed to be able to understand spatial languages to collaborate and cooperate with humans. However, it remains unknown whether human speakers regard robots as human-like social partners. In this study, human speakers describe target locations to an imaginary human or robot addressee under various scenarios varying in relative speaker–addressee cognitive burden. Speakers made equivalent perspective choices to human and robot addressees, which consistently shifted according to the relative speaker–addressee cognitive burden. However, speakers’ perspective choice was only significantly correlated to their social skills when the addressees were humans but not robots. These results suggested that people generally assume robots and humans with equal capabilities in understanding spatial descriptions but do not regard robots as human-like social partners.},
   author = {Chengli Xiao and Liufei Xu and Yuqing Sui and Renlai Zhou},
   doi = {10.3389/fpsyg.2020.578244},
   issn = {16641078},
   journal = {Frontiers in Psychology},
   keywords = {human–robot interaction,perspective-taking,social cognition,social skills,spatial cognition,spatial descriptions},
   month = {2},
   publisher = {Frontiers Media S.A.},
   title = {Do People Regard Robots as Human-Like Social Partners? Evidence From Perspective-Taking in Spatial Descriptions},
   volume = {11},
   year = {2021},
}
@article{Lakatos2021,
   abstract = {In this work, we tested a recently developed novel methodology to assist children with Autism Spectrum Disorder (ASD) improve their Visual Perspective Taking (VPT) and Theory of Mind (ToM) skills using the humanoid robot Kaspar. VPT is the ability to see the world from another person's perspective, drawing upon both social and spatial information. Children with ASD often find it difficultto understand that others might have perspectives, viewpoints and beliefs that are different from their own, which is a fundamental aspect of both VPT and ToM. The games we designed were implementedas the first attempt to study if these skills can be improved in children with ASD through interacting with a humanoid robot in a series of trials. The games involved a number of different actions with the common goal of helping the children to see the world from the robot's perspective. Childrenwith ASD were recruited to the study according to specific inclusion criteria that were determined in a previous pilot study. In order to measure the potential impact of the games on the children, three pre- and post-tests (Smarties, Sally-Anne and Charlie tests) were conducted with the children Our findings suggest that children with ASD can indeed benefit from this approach of robot-assistedtherapy.},
   author = {Gabriella Lakatos and Luke Jai Wood and Dag Sverre Syrdal and Ben Robins and Abolfazl Zaraki and Kerstin Dautenhahn},
   doi = {10.1515/pjbr-2021-0007},
   issn = {20814836},
   issue = {1},
   journal = {Paladyn},
   keywords = {assistive robotics,autism,human-robot interaction,theory of mind,visual perspective taking},
   month = {1},
   note = {Study: ASD Children shown what the perspective of the Kasper robot is (literally). The robot has a camera so the children can be shown feedback of what the robot is 'seeing'.<br/><br/>Discussion: Seemingly successful as an intervention but no control group, <br/><br/>Misc Notes:<br/>VPT1: Can understand what another agent can and cannot see.<br/>VPT2: Can understand that object from different positions looks differently? - not sure about this description<br/><br/>VPT thought to be a components of ToM.},
   pages = {87-101},
   publisher = {De Gruyter Open Ltd},
   title = {Robot-mediated intervention can assist children with autism to develop visual perspective taking skills},
   volume = {12},
   year = {2021},
}

@ARTICLE{Liu2022,
  author={Liu, Mingjiang and Xiao, Chengli and Chen, Chunlin},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={Perspective-Corrected Spatial Referring Expression Generation for Human–Robot Interaction}, 
  year={2022},
  volume={52},
  number={12},
  pages={7654-7666},
  doi={10.1109/TSMC.2022.3161588}}

@article{Chen2021,
   abstract = {In order to engage in complex social interaction, humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict others' plans and behaviors. These abilities have been mostly lacking in robots, sometimes making them appear awkward and socially inept. Here we propose an end-to-end long-term visual prediction framework for robots to begin to acquire both these critical cognitive skills, known as Visual Perspective Taking (VPT) and Theory of Behavior (TOB). We demonstrate our approach in the context of visual hide-and-seek - a game that represents a cognitive milestone in human development. Unlike traditional visual predictive model that generates new frames from immediate past frames, our agent can directly predict to multiple future timestamps (25s), extrapolating by 175% beyond the training horizon. We suggest that visual behavior modeling and perspective taking skills will play a critical role in the ability of physical robots to fully integrate into real-world multi-agent activities. Our website is at http://www.cs.columbia.edu/~bchen/vpttob/.},
   author = {Boyuan Chen and Yuhang Hu and Robert Kwiatkowski and Shuran Song and Hod Lipson},
   month = {5},
   title = {Visual Perspective Taking for Opponent Behavior Modeling},
   url = {http://arxiv.org/abs/2105.05145},
   year = {2021},
    journal = {2021 IEEE International Conference on Robotics and Automation (ICRA 2021)},
   pages = {13678-13685},

    
}


@article{Samuel2021,
   abstract = {The ability to represent another agent's visual perspective has recently been attributed to a process called “perceptual simulation”, whereby we generate an image-like or “quasi-perceptual” representation of another agent's vision. In an extensive series of experiments we tested this notion. Adult observers were presented with pictures of an agent looking at two horizontal lines, one of which was closer to the agent and hence appeared longer from his/her visual perspective. In each case approximately as many participants judged the closer line to appear shorter as longer (to the agent), i.e., failures to take the agent's perspective. This occurred when clear depth cues were added to emphasise the agent's location relative to the stimuli, when the agent was moved closer to the lines, when the lines were oriented vertically, when judgments could be made while viewing the image, and when participants imagined themselves in the agent's place. It also persisted when we asked participants to imagine what a photo taken from the same location as the agent would show, ruling out a misinterpretation of the instructions. Overall, our data suggest that adults attempt to solve visual perspective-taking problems by drawing upon naïve and often erroneous ideas about how vision works.},
   author = {Steven Samuel and Klara Hagspiel and Madeline J. Eacott and Geoff G. Cole},
   doi = {10.1016/j.cognition.2021.104607},
   issn = {18737838},
   journal = {Cognition},
   keywords = {Naïve optics,Perceptual simulation,Perspective-taking,Theory of mind,Vision},
   month = {5},
   pmid = {33508578},
   publisher = {Elsevier B.V.},
   title = {Visual perspective-taking and image-like representations: We don't see it},
   volume = {210},
   year = {2021},
}

@ARTICLE{Fischer2020,
  author={Fischer, Tobias and Demiris, Yiannis},
  journal={IEEE Transactions on Cognitive and Developmental Systems}, 
  title={Computational Modeling of Embodied Visual Perspective Taking}, 
  year={2020},
  volume={12},
  number={4},
  pages={723-732},
  doi={10.1109/TCDS.2019.2949861}}

@article{MaCdorman2013,
   abstract = {When a computer-animated human character looks eerily realistic, viewers report a loss of empathy; they have difficulty taking the character's perspective. To explain this perspective-taking impairment, known as the uncanny valley, a novel theory is proposed: The more human or less eerie a character looks, the more it interferes with level 1 visual perspective taking when the character's perspective differs from that of the human observer (e.g., because the character competitively activates shared circuits in the observer's brain). The proposed theory is evaluated in three experiments involving a dot-counting task in which participants either assumed or ignored the perspective of characters varying in their human photorealism and eeriness. Although response times and error rates were lower when the number of dots faced by the observer and character were the same (congruent condition) than when they were different (incongruent condition), no consistent pattern emerged between the human photorealism or eeriness of the characters and participants' response times and error rates. Thus, the proposed theory is unsupported for level 1 visual perspective taking. As the effects of the uncanny valley on empathy have not previously been investigated systematically, these results provide evidence to eliminate one possible explanation.© 2013 Elsevier Ltd. All rights reserved.},
   author = {Karl F. MaCdorman and Preethi Srinivas and Himalaya Patel},
   doi = {10.1016/j.chb.2013.01.051},
   issn = {07475632},
   issue = {4},
   journal = {Computers in Human Behavior},
   keywords = {Anthropomorphism,Character animation,Cognitive empathy,Mirror neuron system,Theory of mind},
   pages = {1671-1685},
   title = {The uncanny valley does not interfere with level 1 visual perspective taking},
   volume = {29},
   year = {2013},
}
@article{Xiao2022,
   abstract = {Taking the perspective of others is critical for both human–human and human–robot interactions. Previous studies using the dot perspective task have revealed that people could automatically process what other people can see. In this study, following the classical dot perspective task, we showed that Chinese participants could not automatically process humanoid robot avatars’ perspective when only judging from self-perspective (Experiment 1) or randomly judging between self and avatar’s perspectives (Experiment 2), and people’s anthropomorphism tendency was related to the efficiency but not the automaticity of perspective-taking. These results revealed that human–human and human–robot interactions might be different in the basic visual process, and suggested the anthropomorphism tendency in people as an influential factor in human–robot interaction.},
   author = {Chengli Xiao and Ya Fan and Jingyu Zhang and Renlai Zhou},
   doi = {10.1007/s12369-021-00773-x},
   issn = {18754805},
   issue = {1},
   journal = {International Journal of Social Robotics},
   keywords = {Anthropomorphism,Human–robot interaction,Individual differences,Visual perspective‐taking},
   month = {1},
   note = {Misc: People take VPT1 of robots when giving them instructions, or watching a robot sitting nect to a target object.},
   pages = {165-176},
   publisher = {Springer Science and Business Media B.V.},
   title = {People Do not Automatically Take the Level-1 Visual Perspective of Humanoid Robot Avatars},
   volume = {14},
   year = {2022},
}
@article{Ward2019,
   abstract = {Visual perspective taking (VPT) is a core process of social cognition, providing humans with insights into what the environment looks like from another's point of view [1–4]. While VPT is often described as a quasi-perceptual phenomenon [5, 6], evidence for this proposal has been lacking. Here, we provide direct evidence that another's perspective can “stand in” for one's own sensory input during perceptual decision making. In a variant of the classic mental rotation task, participants judged whether characters presented in different orientations were canonical or mirror inverted. In the absence of another person, we replicate the well-established positive linear relationship between recognition times and angle of orientation such that recognition becomes slower the more an item has to be mentally rotated into its canonical orientation [7]. Importantly, this relationship was disrupted simply by placing another individual in the scene. Items rotated away from the participant were recognized more rapidly the closer they appeared in their canonical orientation, not only to the participant, but also to this other individual, showing that another's visual perspective drives mental rotation and item recognition in a similar way as one's own visual perspective. The effects were large and replicated in the three independent studies. They were observed even when the other person was completely passive, enhanced when the participant was explicitly instructed to take the other person's perspective, but reduced when the persons in the scenes were replaced with objects. The content of another's perspective is therefore spontaneously derived, takes a quasi-perceptual form, and can stand in for one's own sensory input during perceptual decision making. Ward et al. use a mental rotation task to show that another's visual perspective is represented in a (quasi-)perceptual format and can stand in for one's own visual input, allowing items rotated away from oneself to be more quickly identified when appearing upright to an incidentally presented other person (but not to matched “mindless” objects).},
   author = {Eleanor Ward and Giorgio Ganis and Patric Bach},
   doi = {10.1016/j.cub.2019.01.046},
   issn = {09609822},
   issue = {5},
   journal = {Current Biology},
   keywords = {mental imagery,mental rotation,perceptual simulation,perpetual decision making,perspective taking,social interaction,social perception,spatial reference frames,theory of mind,visual perspective taking},
   month = {3},
   pages = {874-880.e4},
   pmid = {30799242},
   publisher = {Cell Press},
   title = {Spontaneous Vicarious Perception of the Content of Another's Visual Perspective},
   volume = {29},
   year = {2019},
}

@article{Ward2020,
   abstract = {Other peoples' (imagined) visual perspectives are represented perceptually in a similar way to our own, and can drive bottom-up processes in the same way as own perceptual input (Ward, Ganis, & Bach, 2019). Here we test directly whether visual perspective taking is driven by where another person is looking, or whether these perceptual simulations represent their position in space more generally. Across two experiments, we asked participants to identify whether alphanumeric characters, presented at one of eight possible orientations away from upright, were presented normally, or in their mirror-inverted form (e.g. “R” vs. “Я”). In some scenes, a person would appear sitting to the left or the right of the participant. We manipulated either between-trials (Experiment 1) or between-subjects (Experiment 2), the gaze-direction of the inserted person, such that they either (1) looked towards the to-be-judged item, (2) averted their gaze away from the participant, or (3) gazed out towards the participant (Exp. 2 only). In the absence of another person, we replicated the well-established mental rotation effect, where recognition of items becomes slower the more items are oriented away from upright (e.g. Shepard and Meltzer, 1971). Crucially, in both experiments and in all conditions, this response pattern changed when another person was inserted into the scene. People spontaneously took the perspective of the other person and made faster judgements about the presented items in their presence if the characters were oriented towards upright to them. The gaze direction of this other person did not influence these effects. We propose that visual perspective taking is therefore a general spatial-navigational ability, allowing us to calculate more easily how a scene would (in principle) look from another position in space, and that such calculations reflect the spatial location of another person, but not their gaze.},
   author = {Eleanor Ward and Giorgio Ganis and Katrina L. McDonough and Patric Bach},
   doi = {10.1016/j.cognition.2020.104241},
   issn = {18737838},
   journal = {Cognition},
   keywords = {Gaze cuing,Mental imagery,Mental rotation,Navigation,Perceptual simulation,Visual perspective taking},
   month = {6},
   pmid = {32105910},
   publisher = {Elsevier B.V.},
   title = {Perspective taking as virtual navigation? Perceptual simulation of what others see reflects their location in space but not their gaze},
   volume = {199},
   year = {2020},
}

@article{Ward2022,
   abstract = {Visual perspective taking may rely on the ability to mentally rotate one’s own body into that of another. Here, we test whether participants’ ability to make active body movements plays a causal role in visual perspective taking. We utilised our recent task that measures whether participants spontaneously represent another’s visual perspective in a (quasi-)perceptual format that can drive own perceptual decision making. Participants reported whether alphanumeric characters, presented in different orientations, are shown in their normal or mirror-inverted form (e.g., “R” vs. “Я”). Between trials, we manipulated whether another person was sitting either left or right of the character and whether participants’ movement was restricted with a chinrest or whether they could move freely. As in our previous research, participants spontaneously took the visual perspective of the other person, recognising rotated letters more rapidly when they appeared upright to the other person in the scene, compared with when they faced away from that person, and these effects increased with age but were (weakly) negatively related to schizotypy and not to autistic traits or social skills. Restricting participants’ ability to make active body movements did not influence these effects. The results, therefore, rule out that active physical movement plays a causal role in computing another’s visual perspective, either to create alignment between own and other’s perspective or to trigger perspective taking processes. The postural adjustments people sometimes make when making judgements from another’s perspective may instead be a bodily consequence of mentally transforming one’s actual to an imagined position in space.},
   author = {Eleanor Ward and Giorgio Ganis and Katrina L. McDonough and Patric Bach},
   doi = {10.1177/17470218221077102},
   issn = {17470226},
   issue = {7},
   journal = {Quarterly Journal of Experimental Psychology},
   keywords = {Perspective-taking,active inference,mental imagery,mental rotation,mentalising,navigation,perceptual simulation,submentalising,visual perspective taking},
   month = {7},
   pages = {1244-1258},
   pmid = {35040382},
   publisher = {SAGE Publications Ltd},
   title = {Is implicit Level-2 visual perspective-taking embodied? Spontaneous perceptual simulation of others’ perspectives is not impaired by motor restriction},
   volume = {75},
   year = {2022},
}

@article{natarajan2023human,
  title={Human-robot teaming: grand challenges},
  author={Natarajan, Manisha and Seraj, Esmaeil and Altundas, Batuhan and Paleja, Rohan and Ye, Sean and Chen, Letian and Jensen, Reed and Chang, Kimberlee Chestnut and Gombolay, Matthew},
  journal={Current Robotics Reports},
  volume={4},
  number={3},
  pages={81--100},
  year={2023},
  publisher={Springer}
}

@INPROCEEDINGS{Brosque2020,
  author={Brosque, Cynthia and Galbally, Elena and Khatib, Oussama and Fischer, Martin},
  booktitle={2020 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)}, 
  title={Human-Robot Collaboration in Construction: Opportunities and Challenges}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/HORA49412.2020.9152888},
publisher = {IEEE},
address = {New York, NY, United States}
}


@article{Inkulu2022,
author={Inkulu,Anil K. and MVA,Raju B. and Dara,Ashok and SankaranarayanaSamy,K.},
year={2022},
title={Challenges and opportunities in human robot collaboration context of Industry 4.0 - a state of the art review},
journal={The Industrial Robot},
volume={49},
number={2},
pages={226-239},
note={Copyright - © Emerald Publishing Limited; Last updated - 2023-09-11},
abstract={Purpose>In the present era of Industry 4.0, the manufacturing automation is moving toward mass production and mass customization through human–robot collaboration. The purpose of this paper is to describe various human–robot collaborative (HRC) techniques and their applicability for various manufacturing methods along with key challenges.Design/methodology/approach>Numerous recent relevant research literature has been analyzed, and various human–robot interaction methods have been identified, and detailed discussions are made on one- and two-way human–robot collaboration.Findings>The challenges in implementing human–robot collaboration for various manufacturing process and the challenges in one- and two-way collaboration between human and robot are found and discussed.Originality/value>The authors have attempted to classify the HRC techniques and demonstrated the challenges in different modes.},
keywords={Computers--Artificial Intelligence; Industry 4.0; HRI; Mass production; Mass customization; Robots; Automation; Industrial applications; Collaboration; Manufacturing; Production methods; Sensors; State-of-the-art reviews; Robotics},
isbn={0143991X},
language={English},
url={https://www.proquest.com/scholarly-journals/challenges-opportunities-human-robot/docview/2626883272/se-2},
}

@article{Kozhevnikov2006,
author = {Kozhevnikov, Maria and Motes, Michael A. and Rasch, Bjoern and Blajenkova, Olessia},
title = {Perspective-taking vs. mental rotation transformations and how they predict spatial navigation performance},
journal = {Applied Cognitive Psychology},
volume = {20},
number = {3},
pages = {397-417},
doi = {https://doi.org/10.1002/acp.1192},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/acp.1192},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/acp.1192},
abstract = {Abstract In Experiment 1, participants completed one of two versions of a computerized pointing direction task that used the same stimuli but different spatial transformation instructions. In the perspective-taking version, participants were to imagine standing at one location facing a second location and then to imagine pointing to a third location. In the array-rotation version, participants saw a vector pointing to one location, were to imagine the second vector with the same base as the first pointing to a second location, to mentally rotate the two vectors, and finally to indicate the direction of the imagined vector after the rotation. In Experiment 2, participants completed the perspective-taking, mental rotation, and four large-scale navigational tasks. The results showed that the perspective-taking task required unique spatial transformation ability from the array rotation task, and the perspective-taking task predicted unique variance over the mental rotation task in navigational tasks that required updating self-to-object representations. Copyright © 2006 John Wiley \& Sons, Ltd.},
year = {2006}
}

@article{Freundlieb2016,
  title={When do humans spontaneously adopt another’s visuospatial perspective?},
  author={Freundlieb, Martin and Kov{\'a}cs, {\'A}gnes M and Sebanz, Natalie},
  journal={Journal of experimental psychology: human perception and performance},
  volume={42},
  number={3},
  pages={401},
  year={2016},
  publisher={American Psychological Association}
}

@article{Bach2014,
  title={Can’t touch this: The first-person perspective provides privileged access to predictions of sensory action outcomes.},
  author={Bach, Patric and Fenton-Adams, Wendy and Tipper, Steven P},
  journal={Journal of Experimental Psychology: Human Perception and Performance},
  volume={40},
  number={2},
  pages={457},
  year={2014},
  publisher={American Psychological Association}
}

@article{Creem-Regehr2013,
  title={Relating spatial perspective taking to the perception of other's affordances: Providing a foundation for predicting the future behavior of others},
  author={Creem-Regehr, Sarah H and Gagnon, Kyle T and Geuss, Michael N and Stefanucci, Jeanine K},
  journal={Frontiers in human neuroscience},
  volume={7},
  pages={596},
  year={2013},
  publisher={Frontiers Media SA}
}

@article{Mattan2016,
  title={Empathy and visual perspective-taking performance},
  author={Mattan, Bradley D and Rotshtein, Pia and Quinn, Kimberly A},
  journal={Cognitive neuroscience},
  volume={7},
  number={1-4},
  pages={170--181},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{Furlanetto2016,
  title={Altercentric interference in level 1 visual perspective taking reflects the ascription of mental states, not submentalizing.},
  author={Furlanetto, Tiziano and Becchio, Cristina and Samson, Dana and Apperly, Ian},
  journal={Journal of Experimental Psychology: Human Perception and Performance},
  volume={42},
  number={2},
  pages={158},
  year={2016},
  publisher={American Psychological Association}
}

@article{Batson1997,
  title={Perspective taking: Imagining how another feels versus imaging how you would feel},
  author={Batson, C Daniel and Early, Shannon and Salvarani, Giovanni},
  journal={Personality and social psychology bulletin},
  volume={23},
  number={7},
  pages={751--758},
  year={1997},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{Lemaignan2017,
  title={Artificial cognition for social human--robot interaction: An implementation},
  author={Lemaignan, S{\'e}verin and Warnier, Mathieu and Sisbot, E Akin and Clodic, Aur{\'e}lie and Alami, Rachid},
  journal={Artificial Intelligence},
  volume={247},
  pages={45--69},
  year={2017},
  publisher={Elsevier}
}

@article{Trafton2005,
  title={Enabling effective human-robot interaction using perspective-taking in robots},
  author={Trafton, J Gregory and Cassimatis, Nicholas L and Bugajska, Magdalena D and Brock, Derek P and Mintz, Farilee E and Schultz, Alan C},
  journal={IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans},
  volume={35},
  number={4},
  pages={460--470},
  year={2005},
  publisher={IEEE}
}

@article{Herrmann2007,
author = {Esther Herrmann  and Josep Call  and María Victoria Hernàndez-Lloreda  and Brian Hare  and Michael Tomasello },
title = {Humans Have Evolved Specialized Skills of Social Cognition: The Cultural Intelligence Hypothesis},
journal = {Science},
volume = {317},
number = {5843},
pages = {1360-1366},
year = {2007},
doi = {10.1126/science.1146282},
URL = {https://www.science.org/doi/abs/10.1126/science.1146282},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1146282},
abstract = {Humans have many cognitive skills not possessed by their nearest primate relatives. The cultural intelligence hypothesis argues that this is mainly due to a species-specific set of social-cognitive skills, emerging early in ontogeny, for participating and exchanging knowledge in cultural groups. We tested this hypothesis by giving a comprehensive battery of cognitive tests to large numbers of two of humans' closest primate relatives, chimpanzees and orangutans, as well as to 2.5-year-old human children before literacy and schooling. Supporting the cultural intelligence hypothesis and contradicting the hypothesis that humans simply have more “general intelligence,” we found that the children and chimpanzees had very similar cognitive skills for dealing with the physical world but that the children had more sophisticated cognitive skills than either of the ape species for dealing with the social world.}}

@article{Heyes2014,
author = {Cecilia M. Heyes  and Chris D. Frith },
title = {The cultural evolution of mind reading},
journal = {Science},
volume = {344},
number = {6190},
pages = {1243091},
year = {2014},
doi = {10.1126/science.1243091},
URL = {https://www.science.org/doi/abs/10.1126/science.1243091},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1243091},
abstract = {No parent needs reminding that children are born with a surprising set of abilities. But children still need many hours of guidance and instruction. Heyes and Frith review one particular social cognitive skill: reading the minds of others (or at least working out what other people are thinking and feeling). An unrefined capacity for “mind reading” is present in infants, but teaching is necessary to develop the full-blown capacity seen in adults. The authors draw parallels between learning to read and learning to read minds. Science, this issue p. 10.1126/science.1243091 It is not just a manner of speaking: “Mind reading,” or working out what others are thinking and feeling, is markedly similar to print reading. Both of these distinctly human skills recover meaning from signs, depend on dedicated cortical areas, are subject to genetically heritable disorders, show cultural variation around a universal core, and regulate how people behave. But when it comes to development, the evidence is conflicting. Some studies show that, like learning to read print, learning to read minds is a long, hard process that depends on tuition. Others indicate that even very young, nonliterate infants are already capable of mind reading. Here, we propose a resolution to this conflict. We suggest that infants are equipped with neurocognitive mechanisms that yield accurate expectations about behavior (“automatic” or “implicit” mind reading), whereas “explicit” mind reading, like literacy, is a culturally inherited skill; it is passed from one generation to the next by verbal instruction.}}

@inproceedings{Almeida2023,
  title={Would You Help Me? Linking Robot's Perspective-Taking to Human Prosocial Behavior},
  author={Almeida, Jo{\~a}o Tiago and Leite, Iolanda and Yadollahi, Elmira},
  booktitle={Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={388--397},
  year={2023},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, United States}
}

@inproceedings{Zhao2015,
author = {Zhao, Xuan and Cusimano, Corey and Malle, Bertram F.},
title = {Do People Spontaneously Take a Robot's Visual Perspective?},
year = {2015},
isbn = {9781450333184},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701973.2702044},
doi = {10.1145/2701973.2702044},
abstract = {This study takes a novel approach to the topic of perspective taking in HRI. In a human behavioral experiment, we examined whether and in what circumstances people spontaneously take a humanoid robot's visual perspective. We found that specific nonverbal behaviors displayed by a robot--namely, referential gaze and goal-directed reaching--led human viewers to take the robot's visual perspective, though marginally less frequently than when they encounter the same behaviors displayed by another human. This project identifies specific features of robot behavior that trigger spontaneous social-cognitive processes in human viewers and informs the design of interactive robots in the future.},
booktitle = {Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts},
pages = {133–134},
numpages = {2},
keywords = {human-robot interaction (HRI), perspective taking, humanoid robot, nonverbal behaviors, communication},
location = {Portland, Oregon, USA},
series = {HRI'15 Extended Abstracts}
}


@article{Wahn2023a,
  title={Try to see it my way: Humans take the level-1 visual perspective of humanoid robot avatars},
  author={Wahn, Basil and Berio, Leda and Wei{\ss}, Matthias and Newen, Albert},
  journal={International Journal of Social Robotics},
  pages={1--12},
  year={2023},
  publisher={Springer}
}

@article{Wahn2023b,
  title={The influence of robot appearance on visual perspective taking: Testing the boundaries of the mere-appearance hypothesis},
  author={Wahn, Basil and Berio, Leda},
  journal={Consciousness and Cognition},
  volume={116},
  pages={103588},
  year={2023},
  publisher={Elsevier}
}

@article{Ye2023,
  title={Human-like interactions prompt people to take a robot’s perspective},
  author={Ye, Tian and Minato, Takashi and Sakai, Kurima and Sumioka, Hidenobu and Hamilton, Antonia and Ishiguro, Hiroshi},
  journal={Frontiers in Psychology},
  volume={14},
  year={2023},
  publisher={Frontiers Media SA}
}

@incollection{telenoid,
  title={Telenoid: Tele-presence android for communication},
  author={Ogawa, Kohei and Nishio, Shuichi and Koda, Kensuke and Taura, Koichi and Minato, Takashi and Ishii, Carlos Toshinori and Ishiguro, Hiroshi},
  booktitle={ACM SIGGRAPH 2011 emerging technologies},
  pages={1--1},
  year={2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, United States}
}

@article{Surtees2012,
  title={Direct and indirect measures of level-2 perspective-taking in children and adults},
  author={Surtees, Andrew DR and Butterfill, Stephen A and Apperly, Ian A},
  journal={British Journal of Developmental Psychology},
  volume={30},
  number={1},
  pages={75--86},
  year={2012},
  publisher={Wiley Online Library}
}



@article{Samson2010,
  title={Seeing it their way: evidence for rapid and involuntary computation of what other people see.},
  author={Samson, Dana and Apperly, Ian A and Braithwaite, Jason J and Andrews, Benjamin J and Bodley Scott, Sarah E},
  journal={Journal of Experimental Psychology: Human Perception and Performance},
  volume={36},
  number={5},
  pages={1255},
  year={2010},
  publisher={American Psychological Association}
}

@article{Zhao2022,
  title={Spontaneous perspective taking toward robots: The unique impact of humanlike appearance},
  author={Zhao, Xuan and Malle, Bertram F},
  journal={Cognition},
  volume={224},
  pages={105076},
  year={2022},
  publisher={Elsevier}
}


@article{Bukowski2015,
  title={From gaze cueing to perspective taking: Revisiting the claim that we automatically compute where or what other people are looking at},
  author={Bukowski, Henryk and Hietanen, Jari K and Samson, Dana},
  journal={Visual cognition},
  volume={23},
  number={8},
  pages={1020--1042},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{Shepard1971,
  title={Mental rotation of three-dimensional objects},
  author={Shepard, Roger N and Metzler, Jacqueline},
  journal={Science},
  volume={171},
  number={3972},
  pages={701--703},
  year={1971},
  publisher={American Association for the Advancement of Science}
}

@misc{Inquisit6_2022,
  author = {millisecond},
  title = {Inquisit6},
  url   = {https://www.millisecond.com/},
  year  = {2022},
}

@ARTICLE{Pepper_ref,
  author={Pandey, Amit Kumar and Gelin, Rodolphe},
  journal={IEEE Robotics \& Automation Magazine}, 
  title={A Mass-Produced Sociable Humanoid Robot: Pepper: The First Machine of Its Kind}, 
  year={2018},
  volume={25},
  number={3},
  pages={40-48},
  doi={10.1109/MRA.2018.2833157}}

@article{SimonEffect,
  title={The role of attention for the Simon effect},
  author={Hommel, Bernhard},
  journal={Psychological research},
  volume={55},
  number={3},
  pages={208--222},
  year={1993},
  publisher={Springer}
}

@book{Python2-7,
  title={Python reference manual},
  author={Van Rossum, Guido and Drake Jr, Fred L},
  year={1995},
  publisher={Centrum voor Wiskunde en Informatica Amsterdam}
}

@article{Michalos2022,
  title={Human robot collaboration in industrial environments},
  author={Michalos, George and Karagiannis, Panagiotis and Dimitropoulos, Nikos and Andronas, Dionisis and Makris, Sotiris},
  journal={The 21st century industrial robot: when tools become collaborators},
  pages={17--39},
  year={2022},
  publisher={Springer}
}

@article{Matheson2019,
  title={Human--robot collaboration in manufacturing applications: A review},
  author={Matheson, Eloise and Minto, Riccardo and Zampieri, Emanuele GG and Faccio, Maurizio and Rosati, Giulio},
  journal={Robotics},
  volume={8},
  number={4},
  pages={100},
  year={2019},
  publisher={MDPI}
}

@article{Burden2022,
  title={Towards human--robot collaboration in construction: current cobot trends and forecasts},
  author={Burden, Alan G and Caldwell, Glenda Amayo and Guertler, Matthias R},
  journal={Construction Robotics},
  volume={6},
  number={3-4},
  pages={209--220},
  year={2022},
  publisher={Springer}
}

@article{Roesler2023,
  title={Embodiment matters in social hri research: Effectiveness of anthropomorphism on subjective and objective outcomes},
  author={Roesler, Eileen and Manzey, Dietrich and Onnasch, Linda},
  journal={ACM Transactions on Human-Robot Interaction},
  volume={12},
  number={1},
  pages={1--9},
  year={2023},
  publisher={ACM New York, NY}
}

@article{Thellman2022,
  title={Mental state attribution to robots: A systematic review of conceptions, methods, and findings},
  author={Thellman, Sam and de Graaf, Maartje and Ziemke, Tom},
  journal={ACM Transactions on Human-Robot Interaction (THRI)},
  volume={11},
  number={4},
  pages={1--51},
  year={2022},
  publisher={ACM New York, NY}
}

@article{Riek2012,
  title={Wizard of oz studies in hri: a systematic review and new reporting guidelines},
  author={Riek, Laurel D},
  journal={Journal of Human-Robot Interaction},
  volume={1},
  number={1},
  pages={119--136},
  year={2012},
  publisher={Journal of Human-Robot Interaction Steering Committee}
}

@article{Parenti2023,
  title={Theta Synchronization as a Neural Marker of Flexible (re-) use of Socio-cognitive Mechanisms for a New Category of (artificial) Interaction Partners},
  author={Parenti, Lorenzo and Navare, Uma Prashant and Marchesi, Serena and Roselli, Cecilia and Wykowska, Agnieszka},
  journal={Cortex},
  volume={169},
  pages={249--258},
  year={2023},
  publisher={Elsevier}
}

@article{Wykowska2020,
  title={Social robots to test flexibility of human social cognition},
  author={Wykowska, Agnieszka},
  journal={International Journal of Social Robotics},
  volume={12},
  number={6},
  pages={1203--1211},
  year={2020},
  publisher={Springer}
}

@article{ye2021,
  title={Taking the perspectives of many people: Humanization matters},
  author={Ye, Tian and Furumi, Fumikazu and Catarino da Silva, Daniel and Hamilton, Antonia},
  journal={Psychonomic Bulletin \& Review},
  volume={28},
  pages={888--897},
  year={2021},
  publisher={Springer}
}

@article{Zhai2021,
  title={The presence of other-race people disrupts spontaneous level-2 visual perspective taking},
  author={Zhai, Jing and Xie, Jiushu and Chen, Jiahan and Huang, Yujie and Ma, Yuchao and Huang, Yanli},
  journal={Scandinavian Journal of Psychology},
  volume={62},
  number={5},
  pages={655--664},
  year={2021},
  publisher={Wiley Online Library}
}
@article{Barr2013,
title = {Random effects structure for confirmatory hypothesis testing: Keep it maximal},
journal = {Journal of Memory and Language},
volume = {68},
number = {3},
pages = {255-278},
year = {2013},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2012.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X12001180},
author = {Dale J. Barr and Roger Levy and Christoph Scheepers and Harry J. Tily},
keywords = {Linear mixed-effects models, Generalization, Statistics, Monte Carlo simulation},
abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the ‘gold standard’ for confirmatory hypothesis testing in psycholinguistics and beyond.}
}

@article{Leslie2014,
title = {Core mechanisms in ‘theory of mind’},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {12},
pages = {528-533},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304002608},
author = {Alan M. Leslie and Ori Friedman and Tim P. German},
abstract = {Our ability to understand the thoughts and feelings of other people does not initially develop as a theory but as a mechanism. The ‘theory of mind’ mechanism (ToMM) is part of the core architecture of the human brain, and is specialized for learning about mental states. Impaired development of this mechanism can have drastic effects on social learning, seen most strikingly in the autistic spectrum disorders. ToMM kick-starts belief–desire attribution but effective reasoning about belief contents depends on a process of selection by inhibition. This selection process (SP) develops slowly through the preschool period and well beyond. By modeling the ToMM-SP as mechanisms of selective attention, we have uncovered new empirical phenomena. We propose that early ‘theory of mind’ is a modular–heuristic process of domain-specific learning.}
}

@misc{R2013,
  title={R: A language and environment for statistical computing},
  author={R Core Team, R and others},
  year={2013},
  publisher={R foundation for statistical computing Vienna, Austria}
}

@article{Bates2015,
 title={Fitting Linear Mixed-Effects Models Using lme4},
 volume={67},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v067i01},
 doi={10.18637/jss.v067.i01},
 abstract={Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
 number={1},
 journal={Journal of Statistical Software},
 author={Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
 year={2015},
 pages={1–48}
}

@article{bolker2015linear,
  title={Linear and generalized linear mixed models},
  author={Bolker, Benjamin M},
  journal={Ecological statistics: contemporary theory and application},
  pages={309--333},
  year={2015},
  publisher={Oxford University Press Oxford, UK}
}

@inproceedings{Currie2024,
author = {Currie, Joel and Mcdonough, Katrina Louise and Wykowska, Agnieszka and Giannaccini, Maria Elena and Bach, Patric},
title = {More Than Meets the Eye? An Experimental Design to Test Robot Visual Perspective-Taking Facilitators Beyond Mere-Appearance},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640684},
doi = {10.1145/3610978.3640684},
abstract = {Visual Perspective Taking (VPT) underpins human social interaction, from joint action to predicting others' future actions and mentalizing about their goals and affective/mental states. Substantial progress has been made in developing artificial VPT capabilities in robots. However, as conventional VPT tasks rely on the (non-situated, disembodied) presentation of robots on computer screens, it is unclear how a robot's socially reactive and goal-directed behaviours prompt people to take its perspective. We provide a novel experimental paradigm that robustly measures the extent to which human interaction partners take a robot's visual perspective during face-to-face human-robot-interactions, by measuring how much a robot's visual perspective is spontaneously integrated with one's own. The experimental task design of our upcoming user study allows us to investigate the role of robot features beyond its human-like appearance, which have driven research so far, targeting instead its socially reactive behaviour and task engagement with the human interaction partner.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {359–363},
numpages = {5},
keywords = {human-robot interaction, humanoid robot, mind perception, non-verbal behaviours, perspective-taking},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@ARTICLE{Tong2024,
  author={Tong, Yuchuang and Liu, Haotian and Zhang, Zhengtao},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Advancements in Humanoid Robots: A Comprehensive Review and Future Prospects}, 
  year={2024},
  volume={11},
  number={2},
  pages={301-328},
  keywords={Industries;Evolution (biology);Decision making;Humanoid robots;Ontologies;Market research;Next generation networking;Future trends and challenges;humanoid robots;human-robot interaction;key technologies;potential applications},
  doi={10.1109/JAS.2023.124140}}

@article{Brondi2021,
title = {What do we expect from robots? Social representations, attitudes and evaluations of robots in daily life},
journal = {Technology in Society},
volume = {66},
pages = {101663},
year = {2021},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101663},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2100138X},
author = {Sonia Brondi and Monica Pivetti and Silvia {Di Battista} and Mauro Sarrica},
keywords = {Robots, Social representations, Attitudes, Contexts of use, Free-associations},
abstract = {To foresee the potential acceptance, rejection and adaptation of robots in societies, it is necessary to overcome deterministic and linear assumptions and explore the plurality of meanings that shape our relationships with these emerging technologies. With this goal in mind, this study investigates the social representation of robots and its interconnection with attitudes and images, in a convenience sample of young adults in Italy (N = 422). Participants were asked to complete a self-report questionnaire consisting of a free-association task to the word stimulus “robot”, the Robot Attitude Scale, the acceptance of robots in different domains of life and a measure of mind perceptions of robots. The social representation of robots was articulated around three key semantic dimensions opposing: (1) ‘distant/detached’ vs ‘close/integrated’ views; (2) ‘ideal’ vs ‘material’ aspects; (3) assimilation with ‘ICTs’ vs with electric and mechanic ‘devices.’ These three dichotomies defined different positions connected with general attitudes, domain-specific evaluations of robots, and their level of perceived proximity with human beings. In particular, the view of robots as more concrete and integrated objects was related to positive attitudes and acceptance across all considered domains (i.e. Dull/Dirty, Education/Care and Health/Emergency dimensions). In contrast, more distant views were related to negative attitudes. Our study provides insights into how diverse positions could favour or hinder the introduction of robots in different spheres of everyday life.}
}
@article{Putlitz2024,
author = {Johanna zu Putlitz and Eileen Roesler},
title ={Let’s Get Physical: The Influence of Embodiment on Industrial Human-Robot Interaction},

journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {0},
number = {0},
pages = {10711813241264206},
year = {2024},
doi = {10.1177/10711813241264206},

URL = { 
    
        https://doi.org/10.1177/10711813241264206
    
    

},
eprint = { 
    
        https://doi.org/10.1177/10711813241264206
    
    

}
,
    abstract = { In recent years, an increasing number of studies in human-robot interaction (HRI) have used images or videos of robots as a simple, inexpensive, and customizable substitute for physically embodied robots. Thus, the question arises whether results from studies using depictions can be validly applied to interactions with embodied robots. This study investigated the effect of embodiment in HRI within an industrial interaction scenario, focusing on perception, trust, and task performance. Eighty-five participants interacted either with an embodied robot or its depiction via a computer screen. Results showed that the embodied robot was perceived as more likable, intelligent, and safer, eliciting higher initial trust. Primary task performance was observed to be higher in interaction with the embodied robot, while secondary task performance was higher in interaction with the robot’s depiction. These findings challenge the generalizability of depiction-based HRI research, highlighting the importance of considering embodiment in future research designs. }
}

@inproceedings{Bainbridge2008,
  title={The effect of presence on human-robot interaction},
  author={Bainbridge, Wilma A and Hart, Justin and Kim, Elizabeth S and Scassellati, Brian},
  booktitle={RO-MAN 2008-The 17th IEEE International Symposium on Robot and Human Interactive Communication},
  pages={701--706},
  year={2008},
  organization={IEEE}
}

@article{Deng2019,
  title={Embodiment in socially interactive robots},
  author={Deng, Eric and Mutlu, Bilge and Mataric, Maja J and others},
  journal={Foundations and Trends{\textregistered} in Robotics},
  volume={7},
  number={4},
  pages={251--356},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@article{Kiesler2008,
  title={Anthropomorphic interactions with a robot and robot--like agent},
  author={Kiesler, Sara and Powers, Aaron and Fussell, Susan R and Torrey, Cristen},
  journal={Social cognition},
  volume={26},
  number={2},
  pages={169--181},
  year={2008},
  publisher={Guilford Press}
}

@article{Spisak2024,
  title={Diffusing in Someone Else's Shoes: Robotic Perspective Taking with Diffusion},
  author={Spisak, Josua and Kerzel, Matthias and Wermter, Stefan},
  journal={arXiv preprint arXiv:2404.07735},
  year={2024}
}

@article{Todd2024,
  title={Factors that amplify and attenuate egocentric mentalizing},
  author={Todd, Andrew R and Tamir, Diana I},
  journal={Nature Reviews Psychology},
  volume={3},
  number={3},
  pages={164--180},
  year={2024},
  publisher={Nature Publishing Group US New York}
}



@article{
Will2021,
author = {Paris Will  and Elle Merritt  and Rob Jenkins  and Alan Kingstone },
title = {The Medusa effect reveals levels of mind perception in pictures},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {32},
pages = {e2106640118},
year = {2021},
doi = {10.1073/pnas.2106640118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2106640118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2106640118},
abstract = {Differential treatment of animate and inanimate objects often hinges on mind perception—the attribution of mental states to others. It has already been established that pictures of animate objects can elicit mind perception, albeit at reduced intensity. However, this loss of intensity is assumed to reflect an impoverishment of a rich stimulus, such as the projection of a living being into a static picture plane. The current study overturns this assumption by showing that “pure” abstraction can reduce mind perception independent of stimulus richness. Depicting things with minds raises ethical questions that have not been recognized previously. As these questions emerge from representational structure rather than representational content, they are unlikely to be quashed by improvements in image quality. Throughout our species history, humans have created pictures. The resulting picture record reveals an overwhelming preference for depicting things with minds. This preference suggests that pictures capture something of the mind that is significant to us, albeit at reduced potency. Here, we show that abstraction dims the perceived mind, even within the same picture. In a series of experiments, people were perceived as more real, and higher in both Agency (ability to do) and Experience (ability to feel), when they were presented as pictures than when they were presented as pictures of pictures. This pattern persisted across different tasks and even when comparators were matched for identity and image size. Viewers spontaneously discriminated between different levels of abstraction during eye tracking and were less willing to share money with a more abstracted person in a dictator game. Given that mind perception underpins moral judgement, our findings suggest that depicted persons will receive greater or lesser ethical consideration, depending on the level of abstraction.}}

@article{lukovsiunaite2024,
  title={The influence of another’s actions and presence on perspective taking},
  author={Luko{\v{s}}i{\=u}nait{\.e}, Ieva and Kov{\'a}cs, {\'A}gnes M and Sebanz, Natalie},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={4971},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{Surtees2016,
  title={Unintentional perspective-taking calculates whether something is seen, but not how it is seen},
  author={Surtees, Andrew and Samson, Dana and Apperly, Ian},
  journal={Cognition},
  volume={148},
  pages={97--105},
  year={2016},
  publisher={Elsevier}
}

@article{Todd2021,
  title={The goal-dependence of level-1 and level-2 visual perspective calculation.},
  author={Todd, Andrew R and Cameron, C Daryl and Simpson, Austin J},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={47},
  number={6},
  pages={948},
  year={2021},
  publisher={American Psychological Association}
}

@article{Todd2019,
title = {Time pressure disrupts level-2, but not level-1, visual perspective calculation: A process-dissociation analysis},
journal = {Cognition},
volume = {189},
pages = {41-54},
year = {2019},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719300575},
author = {Andrew R. Todd and Austin J. Simpson and C. Daryl Cameron},
keywords = {Altercentrism, Efficiency, Implicit mentalizing, Process dissociation, Theory of mind, Visual perspective taking},
abstract = {Reasoning about other people’s mental states has long been assumed to require active deliberation. Yet, evidence from indirect measures suggests that adults and children commonly display behavior indicative of having incidentally calculated both what other agents see (level-1 perspective taking) and how they see it (level-2 perspective taking). Here, we investigated the efficiency of such perspective calculation in adults. In four experiments using indirect measures of visual perspective taking, we imposed time pressure to constrain processing opportunity, and we used process-dissociation analyses to isolate perspective calculation as the process of focal interest. Results revealed that time pressure weakened level-2, but not level-1, perspective calculation—a pattern that was not evident in error-rate analyses. These findings suggest that perspective calculation may operate more efficiently in level-1 than in level-2 perspective taking. They also highlight the utility of the process-dissociation framework for unmasking processes that otherwise may go under-detected in behavior-level analyses.}
}

@article{Heyes2014a,
  title={Submentalizing: I am not really reading your mind},
  author={Heyes, Cecilia},
  journal={Perspectives on Psychological Science},
  volume={9},
  number={2},
  pages={131--143},
  year={2014},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@inproceedings{Onorati2023,
  title={Creating Personalized Verbal Human-Robot Interactions Using LLM with the Robot Mini},
  author={Onorati, Teresa and Castro-Gonz{\'a}lez, {\'A}lvaro and del Valle, Javier Cruz and D{\'\i}az, Paloma and Castillo, Jos{\'e} Carlos},
  booktitle={International Conference on Ubiquitous Computing and Ambient Intelligence},
  pages={148--159},
  year={2023},
  organization={Springer}
}

@misc{Everett2024,
      title={Capsule Network Projectors are Equivariant and Invariant Learners}, 
      author={Miles Everett and Aiden Durrant and Mingjun Zhong and Georgios Leontidis},
      year={2024},
      eprint={2405.14386},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.14386}, 
}

@inbook{Phillips2018,
author = {Phillips, Elizabeth and Zhao, Xuan and Ullman, Daniel and Malle, Bertram F.},
title = {What is Human-like? Decomposing Robots' Human-like Appearance Using the Anthropomorphic roBOT (ABOT) Database},
year = {2018},
isbn = {9781450349536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3171221.3171268},
abstract = {Anthropomorphic robots, or robots with human-like appearance features such as eyes, hands, or faces, have drawn considerable attention in recent years. To date, what makes a robot appear human-like has been driven by designers» and researchers» intuitions, because a systematic understanding of the range, variety, and relationships among constituent features of anthropomorphic robots is lacking. To fill this gap, we introduce the ABOT (Anthropomorphic roBOT) Database---a collection of 200 images of real-world robots with one or more human-like appearance features (http://www.abotdatabase.info). Harnessing this database, Study 1 uncovered four distinct appearance dimensions (i.e., bundles of features) that characterize a wide spectrum of anthropomorphic robots and Study 2 identified the dimensions and specific features that were most predictive of robots» perceived human-likeness. With data from both studies, we then created an online estimation tool to help researchers predict how human-like a new robot will be perceived given the presence of various appearance features. The present research sheds new light on what makes a robot look human, and makes publicly accessible a powerful new tool for future research on robots» human-likeness.},
booktitle = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {105–113},
numpages = {9}
}

@article{Hartigan1985,
  title={The dip test of unimodality},
  author={Hartigan, John A and Hartigan, Pamela M},
  journal={The annals of Statistics},
  pages={70--84},
  year={1985},
  publisher={JSTOR}
}

@article{Marchesi2022,
	author = {Marchesi, Serena and Tommaso, Davide De and Perez-Osorio, Jairo and Wykowska, Agnieszka},
	journal = {Technology, Mind, and Behavior},
	number = {3: Autumn},
	year = {2022},
	month = {jul 7},
	note = {https://tmb.apaopen.org/pub/56dkj53d},
	publisher = {},
	title = {Belief in {Sharing} the {Same} {Phenomenological} {Experience} {Increases} the {Likelihood} of {Adopting} the {Intentional} {Stance} {Toward} a {Humanoid} {Robot}},
	volume = {3},
}

@article{Tversky2009,
title = {Embodied and disembodied cognition: Spatial perspective-taking},
journal = {Cognition},
volume = {110},
number = {1},
pages = {124-129},
year = {2009},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2008.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0010027708002370},
author = {Barbara Tversky and Bridgette Martin Hard},
keywords = {Perspective-taking, Self, Other, Egocentric, Spatial reasoning},
abstract = {Although people can take spatial perspectives different from their own, it is widely assumed that egocentric perspectives are natural and have primacy. Two studies asked respondents to describe the spatial relations between two objects on a table in photographed scenes; in some versions, a person sitting behind the objects was either looking at or reaching for one of the objects. The mere presence of another person in a position to act on the objects induced a good proportion of respondents to describe the spatial relations from that person’s point of view (Experiment 1). When the query about the spatial relations was phrased in terms of action, more respondents took the other’s perspective than their own (Experiment 2). The implication of action elicits spontaneous spatial perspective-taking, seemingly in the service of understanding the other’s actions.}
}

@article{Cross2021,
  title={Mind meets machine: Towards a cognitive science of human--machine interactions},
  author={Cross, Emily S and Ramsey, Richard},
  journal={Trends in cognitive sciences},
  volume={25},
  number={3},
  pages={200--212},
  year={2021},
  publisher={Elsevier}
}

@article{Schurz2013,
  title={Common brain areas engaged in false belief reasoning and visual perspective taking: a meta-analysis of functional brain imaging studies},
  author={Schurz, Matthias and Aichhorn, Markus and Martin, Anna and Perner, Josef},
  journal={Frontiers in human neuroscience},
  volume={7},
  pages={712},
  year={2013},
  publisher={Frontiers Media SA}
}

@article{Hamilton2009,
  title={Visual perspective taking impairment in children with autistic spectrum disorder},
  author={Hamilton, Antonia F de C and Brindley, Rachel and Frith, Uta},
  journal={Cognition},
  volume={113},
  number={1},
  pages={37--44},
  year={2009},
  publisher={Elsevier}
}

@article{Gunia2021,
  title={Brain mechanisms of visuospatial perspective-taking in relation to object mental rotation and the theory of mind},
  author={Gunia, Anna and Moraresku, Sofiia and Vl{\v{c}}ek, Kamil},
  journal={Behavioural Brain Research},
  volume={407},
  pages={113247},
  year={2021},
  publisher={Elsevier}
}

@article{Metta2010,
  title={The iCub humanoid robot: An open-systems platform for research in cognitive development},
  author={Metta, Giorgio and Natale, Lorenzo and Nori, Francesco and Sandini, Giulio and Vernon, David and Fadiga, Luciano and Von Hofsten, Claes and Rosander, Kerstin and Lopes, Manuel and Santos-Victor, Jos{\'e} and others},
  journal={Neural networks},
  volume={23},
  number={8-9},
  pages={1125--1134},
  year={2010},
  publisher={Elsevier}
}

@article{Kessler2010,
  title={The two forms of visuo-spatial perspective taking are differently embodied and subserve different spatial prepositions},
  author={Kessler, Klaus and Rutherford, Hannah},
  journal={Frontiers in psychology},
  volume={1},
  pages={213},
  year={2010},
  publisher={Frontiers Research Foundation}
}

@article{Moll2013,
  title={The primacy of social over visual perspective-taking},
  author={Moll, Henrike and Kadipasaoglu, Derya},
  journal={Frontiers in human neuroscience},
  volume={7},
  pages={558},
  year={2013},
  publisher={Frontiers Media SA}
}





@Comment{jabref-meta: databaseType:bibtex;}
